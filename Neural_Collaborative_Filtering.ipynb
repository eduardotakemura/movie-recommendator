{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Collaborative Filtering - Movie Recommendator"
      ],
      "metadata": {
        "id": "D_sZstbXc2T3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries and Data Download"
      ],
      "metadata": {
        "id": "RLG2SW4atPbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# External Libraries #\n",
        "!pip install tensorflow\n",
        "\n",
        "# Dataset Download #\n",
        "!wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
        "!unzip ml-latest-small.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRwBuYLoscsw",
        "outputId": "a859615a-7a32-420d-95e8-8ea41bb8e91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "--2024-10-04 12:25:31--  https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
            "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
            "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 978202 (955K) [application/zip]\n",
            "Saving to: ‘ml-latest-small.zip’\n",
            "\n",
            "ml-latest-small.zip 100%[===================>] 955.28K  2.41MB/s    in 0.4s    \n",
            "\n",
            "2024-10-04 12:25:33 (2.41 MB/s) - ‘ml-latest-small.zip’ saved [978202/978202]\n",
            "\n",
            "Archive:  ml-latest-small.zip\n",
            "   creating: ml-latest-small/\n",
            "  inflating: ml-latest-small/links.csv  \n",
            "  inflating: ml-latest-small/tags.csv  \n",
            "  inflating: ml-latest-small/ratings.csv  \n",
            "  inflating: ml-latest-small/README.txt  \n",
            "  inflating: ml-latest-small/movies.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "-_qEMWPs1VBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ramrwp37wlhC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Embedding, Input, Dense, Flatten, Multiply, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "a3v2hjeK3fv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movies = pd.read_csv('ml-latest-small/movies.csv',encoding='utf8')\n",
        "ratings = pd.read_csv('ml-latest-small/ratings.csv',encoding='utf8')"
      ],
      "metadata": {
        "id": "rOs5V_xs5ayF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "QA8SGzYYdmqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Movies Dataset"
      ],
      "metadata": {
        "id": "4pJ-8lA2A9EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _extract_year(title):\n",
        "    # Use regex to capture title and year\n",
        "    match = re.match(r'^(.*)\\s\\((\\d{4})\\)$', title)\n",
        "    if match:\n",
        "        return match.group(1), int(match.group(2))\n",
        "    else:\n",
        "        return title, None\n",
        "\n",
        "df_movies = movies.copy()\n",
        "\n",
        "# Extract release year from the original title and drop it #\n",
        "df_movies[['title', 'year']] = df_movies['title'].apply(lambda x: pd.Series(_extract_year(x)))\n",
        "df_movies.dropna(inplace=True)\n",
        "\n",
        "# Split genres into individual columns #\n",
        "all_genres = set('|'.join(df_movies['genres']).split('|'))\n",
        "for genre in all_genres:\n",
        "    df_movies[genre] = df_movies['genres'].apply(lambda x: 1 if genre in x else 0)\n",
        "df_movies.drop('genres', axis=1, inplace=True)\n",
        "\n",
        "# Scale year column #\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "year_scaler = MinMaxScaler()\n",
        "\n",
        "df_movies['year_normalized'] = year_scaler.fit_transform(df_movies[['year']])"
      ],
      "metadata": {
        "id": "qgcd7oW0-Vo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Ratings and Merge with Movies"
      ],
      "metadata": {
        "id": "AB3tsnGtY4eA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns #\n",
        "df_ratings = ratings[['userId', 'movieId', 'rating']]\n",
        "\n",
        "# Normalize ratings #\n",
        "ratings_scaler = MinMaxScaler()\n",
        "df_ratings['rating_normalized'] = ratings_scaler.fit_transform(df_ratings[['rating']])\n",
        "\n",
        "# Merge the two df #\n",
        "df_merged = pd.merge(df_ratings, df_movies, on='movieId')"
      ],
      "metadata": {
        "id": "tEGi5RpaYwYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode users and movies id, and drop remaining cols"
      ],
      "metadata": {
        "id": "AA8TWxwYvC5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode userId and movieId as categorical values #\n",
        "df_merged['user_encoded'] = df_merged['userId'].astype('category').cat.codes\n",
        "df_merged['movie_encoded'] = df_merged['movieId'].astype('category').cat.codes\n",
        "\n",
        "df_final = df_merged.drop(['userId', 'movieId', 'title', 'year', 'rating'], axis=1)"
      ],
      "metadata": {
        "id": "-IEwNKzUZofn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "IiT-zZISec62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Dataset"
      ],
      "metadata": {
        "id": "hz5f3DOGemJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training, test and validation sets\n",
        "train_val_data, test_data = train_test_split(df_final, test_size=0.2, random_state=42)\n",
        "train_data, val_data = train_test_split(train_val_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare input features (user, movie, movie metadata) and target variable (rating)\n",
        "user_input = train_data['user_encoded'].values\n",
        "movie_input = train_data['movie_encoded'].values\n",
        "movie_features = train_data.drop(columns=['user_encoded', 'movie_encoded', 'rating_normalized']).values\n",
        "ratings = train_data['rating_normalized'].values"
      ],
      "metadata": {
        "id": "IUgpoxr8egyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Outline"
      ],
      "metadata": {
        "id": "-CKJlTI2es4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 64\n",
        "mlp_hidden_units = [128, 64, 32]\n",
        "\n",
        "# Input layers\n",
        "user_input_layer = Input(shape=(1,), name='user_input')\n",
        "movie_input_layer = Input(shape=(1,), name='movie_input')\n",
        "movie_metadata_input = Input(shape=(movie_features.shape[1],), name='movie_metadata_input')\n",
        "\n",
        "# Embedding layers for GMF\n",
        "user_embedding_gmf = Embedding(input_dim=df_final['user_encoded'].nunique(), output_dim=embedding_dim)(user_input_layer)\n",
        "movie_embedding_gmf = Embedding(input_dim=df_final['movie_encoded'].nunique(), output_dim=embedding_dim)(movie_input_layer)\n",
        "\n",
        "# Embedding layers for MLP\n",
        "user_embedding_mlp = Embedding(input_dim=df_final['user_encoded'].nunique(), output_dim=embedding_dim)(user_input_layer)\n",
        "movie_embedding_mlp = Embedding(input_dim=df_final['movie_encoded'].nunique(), output_dim=embedding_dim)(movie_input_layer)\n",
        "\n",
        "# Flatten embeddings\n",
        "user_vec_gmf = Flatten()(user_embedding_gmf)\n",
        "movie_vec_gmf = Flatten()(movie_embedding_gmf)\n",
        "user_vec_mlp = Flatten()(user_embedding_mlp)\n",
        "movie_vec_mlp = Flatten()(movie_embedding_mlp)\n",
        "\n",
        "# GMF branch (element-wise product)\n",
        "gmf_output = Multiply()([user_vec_gmf, movie_vec_gmf])\n",
        "\n",
        "# MLP branch (concatenate embeddings + movie metadata)\n",
        "mlp_input = Concatenate()([user_vec_mlp, movie_vec_mlp, movie_metadata_input])\n",
        "\n",
        "# MLP hidden layers\n",
        "mlp_output = mlp_input\n",
        "for units in mlp_hidden_units:\n",
        "    mlp_output = Dense(units, activation='relu')(mlp_output)\n",
        "\n",
        "# NeuMF (combine GMF and MLP branches)\n",
        "ncf_output = Concatenate()([gmf_output, mlp_output])\n",
        "\n",
        "# Final output layer with sigmoid activation (rating prediction between 0 and 1)\n",
        "final_output = Dense(1, activation='sigmoid', name='prediction')(ncf_output)\n",
        "\n",
        "# Define the model\n",
        "ncf_model = Model(inputs=[user_input_layer, movie_input_layer, movie_metadata_input], outputs=final_output)\n",
        "\n",
        "# Compile the model\n",
        "ncf_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "\n",
        "# Model summary\n",
        "#ncf_model.summary()"
      ],
      "metadata": {
        "id": "ciSegH5nY2k7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "is0n_1IUe3_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping mechanism\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = ncf_model.fit(\n",
        "    [user_input, movie_input, movie_features],\n",
        "    ratings,\n",
        "    epochs=100,\n",
        "    batch_size=64,\n",
        "    validation_data=([val_data['user_encoded'].values, val_data['movie_encoded'].values, val_data.drop(columns=['user_encoded', 'movie_encoded', 'rating_normalized']).values], val_data['rating_normalized'].values),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLsDKjpIe20W",
        "outputId": "193ff3a6-41ce-476f-ee4a-08ffdd76e737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12ms/step - loss: 0.0320 - mae: 0.1370 - val_loss: 0.0352 - val_mae: 0.1436\n",
            "Epoch 2/100\n",
            "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 0.0173 - mae: 0.0981 - val_loss: 0.0366 - val_mae: 0.1465\n",
            "Epoch 3/100\n",
            "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - loss: 0.0077 - mae: 0.0663 - val_loss: 0.0379 - val_mae: 0.1497\n",
            "Epoch 4/100\n",
            "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 12ms/step - loss: 0.0045 - mae: 0.0512 - val_loss: 0.0388 - val_mae: 0.1513\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "k7Z1d7ASfAbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ncf_model.evaluate([test_data['user_encoded'].values, test_data['movie_encoded'].values, test_data.drop(columns=['user_encoded', 'movie_encoded', 'rating_normalized']).values], test_data['rating_normalized'].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8GDMJaxfDFa",
        "outputId": "5f97da9e-a841-4d67-be5c-65100c52ad19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m631/631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.0357 - mae: 0.1450\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03570444881916046, 0.14466847479343414]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model configs"
      ],
      "metadata": {
        "id": "LoruZAHqhvTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model after training\n",
        "ncf_model.save('ncf_model.h5')\n",
        "\n",
        "# Save the movie embedding (GMF part)\n",
        "movie_embedding_gmf = ncf_model.get_layer('embedding_1').get_weights()[0]  # Get the trained embedding weights\n",
        "np.save('movie_embedding_gmf.npy', movie_embedding_gmf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYb0z3-ohxe1",
        "outputId": "144475a4-14de-42ec-a813-5778bbdaf2bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting Movies Ref"
      ],
      "metadata": {
        "id": "bdzozuVMlM7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_movies['movie_encoded'] = df_movies['movieId'].astype('category').cat.codes\n",
        "df_movies.to_csv('movies.csv', index=False)"
      ],
      "metadata": {
        "id": "oOaRyR03jAtB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}